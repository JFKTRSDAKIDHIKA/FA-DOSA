"""
æµ‹è¯•æ˜ å°„-èåˆååŒä¼˜åŒ–åŠŸèƒ½
"""
import torch
import torch.optim as optim

from fa_dosa_demo import (
    HardwareParameters,
    MappingParameters,
    FusionParameters,
    Config,
    ComputationGraph,
    ConditionalPerformanceModel,
    calculate_total_loss_with_hardware_constraints,
)
from onnx_frontend import parse_onnx_to_graph


def test_mapping_fusion_synergy():
    """æµ‹è¯•æ˜ å°„-èåˆååŒä¼˜åŒ–åŠŸèƒ½"""
    print("ğŸ”„ æµ‹è¯•æ˜ å°„-èåˆååŒä¼˜åŒ–åŠŸèƒ½...")
    
    # åˆ›å»ºæµ‹è¯•å›¾
    try:
        graph = parse_onnx_to_graph('simple_cnn.onnx')
        print(f"  âœ“ æˆåŠŸåŠ è½½å›¾: {len(graph.layers)} å±‚, {len(graph.fusion_groups)} ä¸ªèåˆç»„")
    except FileNotFoundError:
        print("  âš ï¸  æœªæ‰¾åˆ° simple_cnn.onnxï¼Œåˆ›å»ºæ¨¡æ‹Ÿå›¾...")
        # åˆ›å»ºç®€å•çš„æµ‹è¯•å›¾
        graph = ComputationGraph()
        
        # æ·»åŠ ä¸¤ä¸ªå·ç§¯å±‚
        graph.add_layer('conv1', {'N': 1, 'C': 3, 'K': 64, 'P': 32, 'Q': 32, 'R': 3, 'S': 3}, 'Conv')
        graph.add_layer('conv2', {'N': 1, 'C': 64, 'K': 128, 'P': 30, 'Q': 30, 'R': 3, 'S': 3}, 'Conv')
        
        # æ·»åŠ ä¸€ä¸ªèåˆç»„
        graph.add_fusion_group(['conv1', 'conv2'])
        
        print(f"  âœ“ åˆ›å»ºæ¨¡æ‹Ÿå›¾: {len(graph.layers)} å±‚, {len(graph.fusion_groups)} ä¸ªèåˆç»„")
    
    # åˆ›å»ºå‚æ•°
    mapping_params = MappingParameters(graph)
    fusion_params = FusionParameters(graph)
    hardware_params = HardwareParameters(initial_num_pes=64, initial_accumulator_kb=64.0, initial_scratchpad_kb=192.0)
    
    # åˆ›å»ºæ€§èƒ½æ¨¡å‹
    model = ConditionalPerformanceModel()
    
    print("\nğŸ“Š åˆå§‹æ€§èƒ½è¯„ä¼°...")
    
    # åˆå§‹å‰å‘ä¼ æ’­
    with torch.no_grad():
        latency, energy, area = model(mapping_params, fusion_params, hardware_params, graph)
        print(f"  åˆå§‹å»¶è¿Ÿ: {latency.item():.6f}")
        print(f"  åˆå§‹èƒ½è€—: {energy.item():.6f}")
        print(f"  åˆå§‹é¢ç§¯: {area.item():.6f}")
    
    # æ‰“å°èåˆæ¦‚ç‡
    print("\nğŸ”€ åˆå§‹èåˆæ¦‚ç‡:")
    for sanitized_key, prob_tensor in fusion_params.fusion_probs.items():
        if hasattr(fusion_params, 'group_name_mapping') and sanitized_key in fusion_params.group_name_mapping:
            original_group = fusion_params.group_name_mapping[sanitized_key]
            group_display = ' + '.join(original_group)
        else:
            group_display = sanitized_key
        prob_value = torch.sigmoid(prob_tensor).item()
        print(f"  {group_display}: {prob_value:.4f}")
    
    # æ‰“å°æ˜ å°„ç­–ç•¥
    print("\nğŸ—ºï¸  åˆå§‹æ˜ å°„ç­–ç•¥:")
    decision_modules = mapping_params.get_all_decision_modules()
    for group_key, decision_module in decision_modules.items():
        print(f"  ç»„ {group_key}:")
        template_probs = decision_module.get_template_probabilities()
        for i, template_name in enumerate(decision_module.template_names):
            prob = template_probs[i].item()
            print(f"    {template_name}: {prob:.4f}")
    
    print("\nğŸš€ å¼€å§‹ååŒä¼˜åŒ–è®­ç»ƒ...")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = optim.Adam(
        list(mapping_params.parameters()) + 
        list(fusion_params.parameters()) + 
        list(hardware_params.parameters()), 
        lr=0.01
    )
    
    # çŸ­æœŸè®­ç»ƒ
    num_epochs = 100
    for epoch in range(num_epochs):
        optimizer.zero_grad()
        
        # è®¡ç®—æŸå¤±
        total_loss = calculate_total_loss_with_hardware_constraints(
            model, mapping_params, fusion_params, hardware_params, graph
        )
        
        # åå‘ä¼ æ’­
        total_loss.backward()
        optimizer.step()
        
        if epoch % 20 == 0:
            print(f"  è½®æ¬¡ {epoch}: æŸå¤± = {total_loss.item():.6f}")
    
    print("\nğŸ“Š ä¼˜åŒ–åæ€§èƒ½è¯„ä¼°...")
    
    # æœ€ç»ˆå‰å‘ä¼ æ’­
    with torch.no_grad():
        final_latency, final_energy, final_area = model(mapping_params, fusion_params, hardware_params, graph)
        print(f"  æœ€ç»ˆå»¶è¿Ÿ: {final_latency.item():.6f}")
        print(f"  æœ€ç»ˆèƒ½è€—: {final_energy.item():.6f}")
        print(f"  æœ€ç»ˆé¢ç§¯: {final_area.item():.6f}")
    
    # æ‰“å°æœ€ç»ˆèåˆæ¦‚ç‡
    print("\nğŸ”€ æœ€ç»ˆèåˆæ¦‚ç‡:")
    for sanitized_key, prob_tensor in fusion_params.fusion_probs.items():
        if hasattr(fusion_params, 'group_name_mapping') and sanitized_key in fusion_params.group_name_mapping:
            original_group = fusion_params.group_name_mapping[sanitized_key]
            group_display = ' + '.join(original_group)
        else:
            group_display = sanitized_key
        prob_value = torch.sigmoid(prob_tensor).item()
        decision = "FUSE" if prob_value > 0.5 else "NO_FUSE"
        print(f"  {group_display}: {prob_value:.4f} â†’ {decision}")
    
    # æ‰“å°æœ€ç»ˆæ˜ å°„ç­–ç•¥
    print("\nğŸ—ºï¸  æœ€ç»ˆæ˜ å°„ç­–ç•¥:")
    for group_key, decision_module in decision_modules.items():
        print(f"  ç»„ {group_key}:")
        template_probs = decision_module.get_template_probabilities()
        for i, template_name in enumerate(decision_module.template_names):
            prob = template_probs[i].item()
            print(f"    {template_name}: {prob:.4f}")
    
    print("\nâœ… æ˜ å°„-èåˆååŒä¼˜åŒ–æµ‹è¯•å®Œæˆ!")


def test_dynamic_fusion_overhead():
    """æµ‹è¯•åŠ¨æ€èåˆå¼€é”€åŠŸèƒ½"""
    print("\nğŸ”§ æµ‹è¯•åŠ¨æ€èåˆå¼€é”€åŠŸèƒ½...")
    
    # åˆ›å»ºç®€å•çš„æµ‹è¯•å›¾
    graph = ComputationGraph()
    graph.add_layer('conv1', {'N': 1, 'C': 64, 'K': 128, 'P': 32, 'Q': 32, 'R': 3, 'S': 3}, 'Conv')
    graph.add_layer('conv2', {'N': 1, 'C': 128, 'K': 256, 'P': 30, 'Q': 30, 'R': 3, 'S': 3}, 'Conv')
    graph.add_fusion_group(['conv1', 'conv2'])
    
    mapping_params = MappingParameters(graph)
    hardware_params = HardwareParameters(initial_num_pes=64, initial_accumulator_kb=64.0, initial_scratchpad_kb=192.0)
    model = ConditionalPerformanceModel()
    
    # è·å–èåˆç»„çš„å†³ç­–æ¨¡å—
    fusion_group = ['conv1', 'conv2']
    decision_module = mapping_params.get_decision_module_for_fusion_group(fusion_group)
    
    print("  ğŸ§ª æµ‹è¯•ä¸åŒæ˜ å°„æ¨¡æ¿çš„èåˆå¼€é”€...")
    
    # æµ‹è¯•æ¯ä¸ªæ¨¡æ¿çš„èåˆå¼€é”€
    for template_name in decision_module.template_names:
        template_instance = decision_module.get_template_by_name(template_name)
        
        # è®¡ç®—éèåˆæˆæœ¬
        non_fusion_latency, non_fusion_energy = model.calculate_group_costs(
            fusion_group, template_instance, hardware_params, graph, 
            is_fusion_calculation=False
        )
        
        # è®¡ç®—èåˆæˆæœ¬ï¼ˆåŠ¨æ€å¼€é”€ï¼‰
        fusion_latency, fusion_energy = model.calculate_group_costs(
            fusion_group, template_instance, hardware_params, graph, 
            is_fusion_calculation=True
        )
        
        # è®¡ç®—å¼€é”€å·®å¼‚
        latency_overhead = fusion_latency - non_fusion_latency
        energy_overhead = fusion_energy - non_fusion_energy
        
        print(f"    {template_name}:")
        print(f"      å»¶è¿Ÿå¼€é”€: {latency_overhead.item():.6f}")
        print(f"      èƒ½è€—å¼€é”€: {energy_overhead.item():.6f}")
        
        # æå– tiling å› å­ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if hasattr(template_instance, 'get_M0'):
            try:
                M0 = template_instance.get_M0().item()
                K0 = template_instance.get_K0().item() if hasattr(template_instance, 'get_K0') else 1.0
                N0 = template_instance.get_N0().item() if hasattr(template_instance, 'get_N0') else 1.0
                print(f"      Tilingå› å­: M0={M0:.2f}, K0={K0:.2f}, N0={N0:.2f}")
            except:
                print(f"      Tilingå› å­: æ— æ³•è·å–")
    
    print("  âœ… åŠ¨æ€èåˆå¼€é”€æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    test_mapping_fusion_synergy()
    test_dynamic_fusion_overhead() 