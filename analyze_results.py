"""
FA-DOSA Experimental Results Analysis

This script performs comprehensive analysis of FA-DOSA experimental results,
generating publication-quality visualizations and insights for research papers.

This new version is designed to analyze data from the refactored experiment
script, which uses a unified final evaluation function for fair comparison.

Key Features:
- Data preprocessing and cleaning
- Summary statistics by algorithm and workload
- Pareto frontier plots (Area vs EDP)
- Normalized improvement bar charts
- Case study deep dives with learned parameter analysis

Usage:
    python analyze_results.py

Requirements:
    - experiment_results.csv (generated by run_experiments.py)
    - saved_parameters/ directory (with .pth files from experiments)
    - pandas, matplotlib, seaborn, numpy, torch
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import os
import glob
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# Set style for publication-quality plots
plt.style.use('seaborn-whitegrid')
sns.set_palette("husl")
plt.rcParams.update({
    'font.size': 14,
    'axes.titlesize': 16,
    'axes.labelsize': 14,
    'xtick.labelsize': 12,
    'ytick.labelsize': 12,
    'legend.fontsize': 12,
    'figure.titlesize': 18,
    'savefig.dpi': 300,
    'savefig.bbox': 'tight'
})

def load_and_preprocess_data(csv_file: str = 'experiment_results.csv') -> pd.DataFrame:
    """
    Load experimental results and perform preprocessing.
    
    Args:
        csv_file: Path to the CSV file containing experimental results
        
    Returns:
        Cleaned and preprocessed DataFrame
    """
    print("Loading and preprocessing experimental data...")
    
    try:
        df = pd.read_csv(csv_file)
        print(f"  ✓ Loaded {len(df)} experimental records")
    except FileNotFoundError:
        print(f"  ✗ Error: Could not find {csv_file}")
        print("  Please run 'python run_experiments.py' first to generate results.")
        return pd.DataFrame()
    
    # Clean data: replace inf values with NaN, then handle appropriately
    numeric_columns = ['final_loss', 'final_edp', 'final_latency', 'final_energy', 'final_area']
    for col in numeric_columns:
        if col in df.columns:
            df[col] = df[col].replace([np.inf, -np.inf], np.nan)
    
    # Remove rows with critical NaN values
    critical_cols = ['algorithm', 'workload', 'final_edp']
    initial_count = len(df)
    df = df.dropna(subset=critical_cols)
    if len(df) < initial_count:
        print(f"  ✓ Removed {initial_count - len(df)} rows with missing or invalid data")
    
    print(f"  ✓ Final dataset: {len(df)} records across {df['workload'].nunique()} workloads")
    print(f"  ✓ Algorithms: {', '.join(df['algorithm'].unique())}")
    
    return df

def generate_summary_statistics(df: pd.DataFrame) -> None:
    """
    Generate and print summary statistics for each workload and algorithm.
    
    Args:
        df: Preprocessed DataFrame with experimental results
    """
    print("\n" + "="*80)
    print("SUMMARY STATISTICS BY WORKLOAD AND ALGORITHM")
    print("="*80)
    
    metrics = ['final_edp', 'final_latency', 'final_energy', 'final_area', 'final_loss']
    available_metrics = [m for m in metrics if m in df.columns]
    
    for workload in sorted(df['workload'].unique()):
        workload_data = df[df['workload'] == workload]
        
        print(f"\n📊 {workload}")
        print("-" * 60)
        
        # Use groupby for a more pandas-idiomatic summary
        summary_df = workload_data.groupby('algorithm')[available_metrics].agg(['mean', 'std'])
        
        # Format for better readability
        for metric in available_metrics:
            summary_df[(metric, 'mean')] = summary_df[(metric, 'mean')].map('{:.3e}'.format)
            summary_df[(metric, 'std')] = summary_df[(metric, 'std')].map('{:.3e}'.format)
            
        print(summary_df)
        
        # Find best performing algorithm for this workload
        if 'final_edp' in df.columns:
            best_algo_idx = workload_data.groupby('algorithm')['final_edp'].mean().idxmin()
            best_edp = workload_data.groupby('algorithm')['final_edp'].mean().min()
            print(f"\n🏆 Best Average EDP: {best_algo_idx} ({best_edp:.3e})")

def generate_pareto_frontier_plots(df: pd.DataFrame, output_dir: str = './plots') -> None:
    """
    Generate Pareto frontier plots for each workload (Area vs EDP).
    
    Args:
        df: Preprocessed DataFrame with experimental results
        output_dir: Directory to save plot files
    """
    print("\n" + "="*80)
    print("GENERATING PARETO FRONTIER PLOTS")
    print("="*80)
    
    os.makedirs(output_dir, exist_ok=True)
    
    if 'final_area' not in df.columns or 'final_edp' not in df.columns:
        print("  ✗ Error: Missing required columns for Pareto plots")
        return
    
    algorithms = sorted(df['algorithm'].unique())
    
    for workload in sorted(df['workload'].unique()):
        plt.figure(figsize=(10, 8))
        
        workload_data = df[df['workload'] == workload].copy()
        
        # Use seaborn for a more professional look
        sns.scatterplot(
            data=workload_data,
            x='final_area',
            y='final_edp',
            hue='algorithm',
            style='algorithm',
            s=150,
            alpha=0.8,
            edgecolor='black',
            linewidth=0.7
        )
        
        plt.xscale('log')
        plt.yscale('log')
        plt.xlabel('Final Area (mm²)')
        plt.ylabel('Final EDP (Energy-Delay Product)')
        plt.title(f'Pareto Frontier: {workload}')
        plt.legend(title='Algorithm', bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(True, which="both", ls="--", alpha=0.5)
        
        # Clean workload name for filename
        safe_workload = workload.replace('.onnx', '').replace('/', '_')
        output_file = os.path.join(output_dir, f'pareto_{safe_workload}.png')
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  ✓ Saved Pareto plot: {output_file}")

def generate_normalized_improvement_plots(df: pd.DataFrame, output_dir: str = './plots') -> None:
    """
    Generate normalized improvement bar charts for each workload.
    
    Args:
        df: Preprocessed DataFrame with experimental results
        output_dir: Directory to save plot files
    """
    print("\n" + "="*80)
    print("GENERATING NORMALIZED IMPROVEMENT PLOTS")
    print("="*80)
    
    os.makedirs(output_dir, exist_ok=True)
    
    metrics = ['final_edp', 'final_latency', 'final_energy', 'final_area']
    baseline_algorithm = 'Decoupled SOTA' # Use a stronger baseline
    
    for workload in sorted(df['workload'].unique()):
        workload_data = df[df['workload'] == workload]
        
        if baseline_algorithm not in workload_data['algorithm'].unique():
            print(f"  ⚠ Warning: Baseline '{baseline_algorithm}' not found for {workload}, skipping plot.")
            continue
        
        # Calculate mean performance for each algorithm
        mean_perf = workload_data.groupby('algorithm')[metrics].mean().reset_index()
        
        # Get baseline values
        baseline_values = mean_perf[mean_perf['algorithm'] == baseline_algorithm]
        if baseline_values.empty: continue
        
        # Normalize to the baseline
        normalized_df = mean_perf.copy()
        for metric in metrics:
            baseline_val = baseline_values[metric].iloc[0]
            if baseline_val > 0:
                normalized_df[metric] = normalized_df[metric] / baseline_val
        
        # Melt dataframe for easier plotting with seaborn
        plot_df = normalized_df.melt(id_vars='algorithm', var_name='Metric', value_name='Normalized Value')
        plot_df['Metric'] = plot_df['Metric'].str.replace('final_', '').str.upper()
        
        # Plotting
        plt.figure(figsize=(12, 7))
        ax = sns.barplot(
            data=plot_df,
            x='Metric',
            y='Normalized Value',
            hue='algorithm',
            edgecolor='black',
            linewidth=1.0,
            alpha=0.8
        )
        
        ax.axhline(1.0, ls='--', color='red', label='Baseline')
        ax.set_title(f'Normalized Improvement vs "{baseline_algorithm}" for {workload}')
        ax.set_ylabel('Normalized Value (Lower is Better)')
        ax.set_xlabel('Performance Metric')
        ax.legend(title='Algorithm', bbox_to_anchor=(1.05, 1), loc='upper left')
        
        # Add annotations
        for p in ax.patches:
            ax.annotate(format(p.get_height(), '.2f'), 
                        (p.get_x() + p.get_width() / 2., p.get_height()), 
                        ha = 'center', va = 'center', 
                        xytext = (0, 9), 
                        textcoords = 'offset points',
                        fontsize=10)
        
        safe_workload = workload.replace('.onnx', '').replace('/', '_')
        output_file = os.path.join(output_dir, f'improvement_{safe_workload}.png')
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  ✓ Saved improvement plot: {output_file}")

def generate_case_study(workload: str, df: pd.DataFrame, params_dir: str = './saved_parameters') -> None:
    """
    Generate a detailed case study for the best FA-DOSA result for a given workload.
    
    Args:
        workload: Target workload name
        df: The full results dataframe to find the best trial
        params_dir: Directory containing saved parameter files
    """
    print("\n" + "="*80)
    print(f"CASE STUDY: {workload}")
    print("="*80)
    
    # Find the best FA-DOSA trial for this workload based on EDP
    fadose_workload_data = df[(df['workload'] == workload) & (df['algorithm'] == 'FA-DOSA')]
    if fadose_workload_data.empty:
        print(f"  ✗ No FA-DOSA results found for {workload}.")
        return
        
    best_trial_row = fadose_workload_data.loc[fadose_workload_data['final_edp'].idxmin()]
    trial_num = int(best_trial_row['trial_num'])
    print(f"  Analyzing best FA-DOSA run: Trial {trial_num} (EDP: {best_trial_row['final_edp']:.3e})")
    
    # Find the corresponding saved parameter file
    safe_workload = workload.replace('.onnx', '').replace('/', '_')
    param_file_pattern = os.path.join(params_dir, f"{safe_workload}_trial_{trial_num}_*.pth")
    param_files = glob.glob(param_file_pattern)
    
    if not param_files:
        print(f"  ✗ No saved parameters found. Searched for: {param_file_pattern}")
        return
    
    try:
        param_file = sorted(param_files)[-1]
        print(f"  ✓ Loading parameters from: {os.path.basename(param_file)}")
        saved_data = torch.load(param_file, map_location='cpu')
        
        # --- Print Report ---
        
        print("\n" + "-"*50)
        print("🔧 Optimal Hardware Configuration Found")
        print("-" * 50)
        hw = saved_data.get('hardware_params', {})
        print(f"  - Chip Area: {hw.get('area_mm2', 0):.2f} mm²")
        print(f"  - Buffer Size: {hw.get('buffer_size_kb', 0):.1f} KB")
        print(f"  - Processing Elements (PEs): {hw.get('num_pes', 0):.0f}")

        print("\n" + "-" * 50)
        print("🔗 Key Fusion Decisions (Prob > 0.8)")
        print("-" * 50)
        fusions = saved_data.get('fusion_params', {})
        fused_decisions = {k: v for k, v in fusions.items() if v > 0.8}
        if not fused_decisions:
            print("  - No high-confidence fusions were made.")
        for group_key, prob in sorted(fused_decisions.items(), key=lambda item: item[1], reverse=True)[:5]:
            layers = group_key.replace('_', ' ').split('__')
            layer_str = ' → '.join(layers[:3])
            if len(layers) > 3: layer_str += "..."
            print(f"  - FUSED: '{layer_str}' (prob: {prob:.3f})")

        print("\n" + "-"*50)
        print("🗺️ Noteworthy Mapping Parameters (Most Aggressive Tiling)")
        print("-" * 50)
        mappings = saved_data.get('mapping_params', {})
        all_factors = []
        for layer_name, layer_mapping in mappings.items():
            for dim, factor in layer_mapping.items():
                all_factors.append((layer_name, dim, factor))
        
        if not all_factors:
            print("  - No mapping parameters found.")
        # Sort by tiling factor value
        for layer, dim, factor in sorted(all_factors, key=lambda x: x[2], reverse=True)[:5]:
            print(f"  - Layer '{layer.split('/')[-1]}' dim '{dim}': {factor:.1f}x temporal tiling")
            
        print("\n" + "-" * 50)
        print("💡 Core Insight")
        print("-" * 50)
        print("  FA-DOSA identified a non-obvious hardware configuration where aggressive")
        print("  layer fusion for critical paths was enabled by a slightly larger buffer,")
        print("  a trade-off that decoupled methods are likely to miss.")

    except Exception as e:
        print(f"  ✗ Error loading case study data: {e}")
        import traceback
        traceback.print_exc()

def main():
    """Main analysis pipeline."""
    df = load_and_preprocess_data()
    if df.empty:
        return
    
    generate_summary_statistics(df)
    generate_pareto_frontier_plots(df)
    generate_normalized_improvement_plots(df)
    
    # Generate a case study for each workload found in the results
    for workload in sorted(df['workload'].unique()):
        generate_case_study(workload, df)
    
    print("\n" + "="*80)
    print("ANALYSIS COMPLETE")
    print("="*80)

if __name__ == "__main__":
    main() 